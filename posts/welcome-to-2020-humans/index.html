



<!DOCTYPE html>
<html>
<head>
	<meta name="viewport" content="width=device-width" />
    <meta charset="utf-8">
    <base href="https://wordbit.com">
    <title> Welcome, humans, to 2020 </title>
    <link rel="canonical" href="https://wordbit.com/posts/welcome-to-2020-humans/">
    

<link rel="stylesheet" href="/css/poole.css">

<link rel="stylesheet" href="/css/lanyon.css">
<link rel="stylesheet" href="/css/custom.css">


</head>


<body class="theme-base-08">
  


<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox"
       xmlns="http://www.w3.org/1999/html" xmlns="http://www.w3.org/1999/html">


<div class="sidebar" id="sidebar">

  <nav class="sidebar-nav">
    <a class="sidebar-nav-item" href="/">Home</a>
    <a class="sidebar-nav-item" href="/posts">All Posts</a>
    <a class="sidebar-nav-item" href="/pages/status">Snippets</a>
    <a class="sidebar-nav-item" href="/pages/about">About</a>
  </nav>

  <div class="sidebar-item">
    <p>
      &copy; 2023. All rights reserved.
    </p>
  </div>
</div>


  
  <div class="wrap">

    <div class="masthead">
  <div class="container">
    <h3 class="masthead-title">
      <a href="/" title="Home">wordbit</a>
    </h3>
  </div>
</div>


    
    <div class="container content">
      <h1 class="post-title">Welcome, humans, to 2020</h1>
        <section id="main">
          <h5 id="date"> Thu Jan 23, 2020 </h5>
          <p>The fun part of starting a new decade is looking back at how much has changed since 2010 and also imagining what life will be like a decade from now in 2030. Covering it all would take several pages, so I’ll just focus on one or two things.</p>

<p>But firstly, in the spirit of “living in the now” (party on Wayne and Garth), I’d like to express my deep appreciation of living in the year 2020.</p>

<p>We are living in the future -  and the future is now. These are remarkable times on cultural, political, and technological fronts - and I wouldn’t want to be anywhere else (chronologically speaking). So here’s to 2020 - it’s going to be one heck of a year.</p>

<p>Ten years ago, I was newly married, childless, and just starting out in my Technical Writing career. Now I’m a grizzled father with grey in my beard and metaphorically-calloused hands that rest confidently on the helm of my writing career. I’m not quite a seasoned Information Developer, but I’m getting there.</p>

<p>In 2010, wordbit.com was only three years old and was still the top hit when you googled “wordbit”. Now you’d be lucky to find my site on page two of the search results. A two-bit Android app for learning languages has usurped my brand, but there’s not much I can do about it. My site, however, is still the top hit on DuckDuckGo - so thanks for that you wonderfully obscure privacy-focused search engine that everyone should use but nobody does.</p>

<p></p>

<p>Also in 2010, Steve Jobs unveiled the iPad. Despite tepid reviews, consumers threw down their money for a shiny piece of the future. <a href="/posts/why-ill-pass-on-the-ipad/">My initial reaction</a> was that I didn’t need one because I didn’t own a laptop (which seems like odd reasoning to me now because owning a laptop is not a prerequisite to owning a tablet - as if these things existed on Maslow’s hierarchy of needs). I even made a bold prediction of sorts:</p>

<blockquote>
<p>Maybe ten years from now, iPads will be as ubiquitous as laptops are today. If so, I may think about picking one up for myself.</p>
</blockquote>

<p>Well, although initial tablet sales outpaced laptop sales for a spell, it is now clear that laptops are here to stay. Android tablets never quite took off - the iPad is still the best tablet out there, despite suffering a bit of an identity crisis. When it comes to productivity, the iPad is not yet a laptop-killer, mostly because the software is not sophisticated enough. The hardware has always been compelling though, and I did cave in 2013 when the thinner iPad Air came out.</p>

<p>Two years later, I bought my first laptop.</p>

<p>Interestingly, it wasn’t the tablet which rose to dominance in the last decade - it was the “phablet”. Of course we don’t call large phones phablets anymore because it is the new normal. <a href="/posts/why-phablets-dont-make-sense/">But back in 2014</a>, I railed against the phablet while expressing my new-found love for tablets. I think my arguments still hold up today, although carriers give you a lot more data these days.</p>

<p>Given how much time people spend on their phones, I get the appeal of a larger screen. I do still prefer the portability of smaller phones though. I’m 100-percent behind shrinking the bezels so that you can get more screen in a smaller form factor. But listen up all phone manufacturers - please stop using small bezels as carte blanche to go even bigger.</p>

<p>And don’t get me started on the notch. Not today. However, <em>Notch Watch</em> will return folks - I promise you that.</p>

<p>So, what will the next decade bring? Hopefully, not 8K anything because most of us haven’t caught up to 4K yet. And hopefully James Cameron doesn’t force us to watch the Avatar sequels in 3D, or try to force 3D on the whole movie industry again, and why did Mercedes Benz create an Avatar car when nobody cares about Avatar and does anybody go to the Avatar theme park and I have so many questions.</p>

<p>I’d like many things by 2030, including world peace, no more Trump, flying cars, and a round Apple Watch, but I don’t think any of that is going to happen.</p>

<p>What I do think is going to happen is that humans will continue to play God until we create an artificial life. Famed futurist Ray Kurzweil believes that <a href="https://www.kurzweilai.net/futurism-the-dawn-of-the-singularity-a-visual-timeline-of-ray-kurzweils-predictions">2029 will be a major inflection point</a> in the development of AI. Specifically, he believes that we will have computers that are on the same level of intelligence as humans. These computers will claim to be sentient and will “openly petition for recognition of this fact”.</p>

<p>This seems like an outlandish prediction, but given the exponential growth of computing power (about double every two years according to Moore’s Law), this claim may not be outside the realm of possibility.</p>

<p>Look how far we’ve come since the birth of artificial intelligence in the late 1960s when <a href="https://en.wikipedia.org/wiki/ELIZA">ELIZA</a>  and the beginnings of the Turing test was birthed. Who knows, in another ten years, you may actually <em>enjoy</em> a conversation with Siri.</p>

<p>Many people are perpetually nervous about their jobs becoming automated, <a href="/posts/the-future-of-technical-writing/">myself included</a>. When I wrote this piece five years ago, I dismissed the result of the automation calculator and stressed the importance of human interaction and interpersonal communication.</p>

<p>However, I’m starting to revise my assessment - in another ten years, I expect that some forms of writing - perhaps even Technical writing - will be seamlessly produced by a software program for use in journalism, marketing, and customer support. The good news is that the job growth for Artificial Intelligence Specialists is <a href="https://www.marketwatch.com/story/the-no-1-job-of-2019-pays-140000-and-its-hiring-growth-has-exploded-74-2019-12-10">exploding year-over-year</a>, which means that if you’re feeling threatened by the upcoming AI revolution, think about how you can transition to a role where you’re working with AI as a tool to improve productivity.</p>

<p>In the meantime, to understand why I believe an artificial intelligence will write as fluently as a human in the near future, you need to experience it for yourself.</p>

<p>When the research lab OpenAI first introduced the GPT-2 transformer in February last year, they withheld the code, saying it was “too dangerous” to release. This text-generating AI system can generate a news story based on a simple prompt. The creators of GPT-2 were worried that malicious parties would spam the internet with millions of fake news articles every minute.</p>

<p>As this fear never quite materialized, they ended up releasing the code, and <a href="https://talktotransformer.com/">you can try it out here</a>. If you aren’t convinced of AI’s authoring potential, play around with GPT-2 for a while and it may just change your mind.</p>

<p>However, if you’re still not convinced, try playing an infinitely-improvising adventure game built on the GPT-2 AI engine. <a href="http://www.aidungeon.io/">AI Dungeon</a> will take you on a surreal narrative all generated on the fly using AI to generate the text. The gameplay is not polished by any means - it feels more like dreaming. But if a narrative-building AI is asleep and dreaming right now, imagine what will happen when it wakes up.</p>

<p>The future application of AI story building in triple-A open-world games or in virtual worlds gives us a clearer path than ever before to the Matrix. Just five years ago, I was <a href="/posts/the-evolution-of-storytelling-in-games/">imagining these sorts of possibilities</a> in gaming narratives, and experiments like AI Dungeon are just the beginning of these possibilities coming true.</p>

<p><a href="/posts/the-intersection-of-vr-and-ai/">I also thought that VR</a> would become the preeminent way of interfacing with AI. Well, the VR hype has died down somewhat since 2015 and I don’t see any radical new screen tech surfacing in the next decade. I might be wrong, but VR and AR may not really take off for another 20 years. Maybe when we’ve moved beyond simply strapping a screen in front of our eyes. And AI interface paradigms such as Elon Musk’s <a href="https://www.neuralink.com/">Neuralink</a> are wildly interesting but so outlandish at this point. Neural implants seem like a natural successor to VR but only in a far-off future in which I will no longer exist.</p>

<p>In the immediate future I expect AI and ambient computing, such as IoT devices, to coalesce into a more cohesive whole. The defragmentation has already begun with the <a href="https://www.theverge.com/2019/12/18/21027890/apple-google-amazon-smart-home-standard-zigbee-connected-ip-project">proposed smart home standard</a> that Apple, Google, and Amazon are agreeing on. It’s time for Alexa, Google, and Siri to become friends instead of arch-enemies.</p>

<p>But not you Bixby. And definitely not Samsung’s <a href="https://www.engadget.com/2020/01/08/neon-artificial-human-avatars-ces-hype-could-not-live-up-to-the-ces-h/">lame AI avatars</a> that surfaced at CES this year.</p>

<p>(While we’re on CES, can I just say that <a href="https://www.cnet.com/news/charmins-pooptime-robot-pal-will-bring-fresh-toilet-roll-when-you-need-it-most/">the robot</a> that will bring you a new roll of toilet paper when you run out was undoubtedly the best thing at CES this year?)</p>

<p>The truth is, I have no idea what’s coming in the new decade, but that’s what makes it exciting. When I think about it, I’m not even sure what defined the last decade.</p>

<p>In the 1980s, we got personal computers. In the 1990s, we got the Internet. In the 2000s, we got the smartphone. What did we get in the 2010s that changed the world?</p>

<p>Perhaps we need more distance. Perhaps one day, we’ll look back at the 2010s and lean over to our best friend (the friend who knows you better than anyone else) and say: “That was the decade you were born, Siri. I can’t believe how far you’ve come.&rdquo;</p>
        </section>
    </div>
  </div>

  <label for="sidebar-checkbox" class="sidebar-toggle"></label>

  

<div class="container">
  <hr />
  
    <span class="left">
    &nbsp; <em>&laquo; Previous:</em> <a class="next" href="https://wordbit.com/posts/what-i-got-wrong/">What I got wrong</a>
    </span>
  

  
    <span class="right">
    <em>Next: </em><a class="next" href="https://wordbit.com/posts/2019-brought-us-more-ways-to-flex/"> &nbsp; 2019 brought us more ways to flex</a> &raquo;
    </span>
  
  <br>
</div>

  <br />


  
  

<div class="container content">
  <footer>
    <div>
      
    </div>
  </footer>
</div>






</body>
</html>




